---
title: La pandémie à travers les lunettes d'un journaliste
author: simon
date: '2020-06-08'
slug: derfel_tweet
categories: []
tags: []
keywords:
  - tech
---
```{r setup, include =F, echo =F}
#
# TODO : valider ceci : Chunk options must be written in one line; no line breaks are allowed inside chunk options;
# https://yihui.name/knitr/options/
knitr::opts_chunk$set(echo = FALSE, 
                      collapse = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      fig.align= "center",
                      fig.width = 12,
                      fig.height = 10,
                      highlight = TRUE,
                      cache = FALSE,
                      cache.lazy = FALSE) # fixes long vecto rnot supported quand on cache des gros éléments https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-error-in-rmd-but-not-in-r-script
```



Le dernier scandale est arrivé car le premier ministre  @francoislegault a bloqué le journaliste @aardon_derfel sur twitter, affirmant que ce dernier le taggait beaucoup trop souvent:  

```{r}
blogdown::shortcode('tweet', '1269321756289925122')
```




@Paul_Laurier a eu la superbe idée de regarder les données pour voir de quoi il en était.  

```{r, eval = FALSE}
#blogdown::shortcode('tweet', '1269726307342000139')
```

***EDIT:2021 Most of the text in this  blog post is broken because Aaron is very prolific tweeter, meaning that I can no longer get tweets from before December 2020 ***

Dans ce post, je m'intéresse aussi au nombre de mentions avant de faire un petit peu de "sentiment analysis" pour s'amuser un peu.

disclaimer:  c'est pas fait pour se prendre au sérieux ce truc.  aussi, c'est pas mal mon premier NLP (natural language processing), donc on va y aller avec la base.  
code disponible sur github, comme d'habitude.  

```{r}
library(tidyverse)
library(rtweet)
library(tidytext)
library(wordcloud)
data(stop_words)
token <- get_tokens()

url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"
raw_twit <- get_timeline("Aaron_Derfel", n =  3200)


twit <- raw_twit %>% 
  mutate(text2 = str_replace(text, url_pattern, ""),  # remove url
         text3 =  str_replace(text2, "^\\d{1,2}\\)", "") # remove thread numbers
         #text4 = str_replace_all(text2, "@([a-zA-Z]|[0-9]|[_])*", "")  # remove @ follwoed by letters , numbers and underscores
         
  ) %>%
  select(-text,-text2) %>%
  rename(text= text3) %>%
  mutate(date = as.Date(created_at))


```


Le journaliste a bien mentionné le premier ministre près de 5 fois par jour depuis le 1er avril avec un pic à 45 mentions en une seule journée.



```{r}
mentions <- twit %>% 
  mutate(
    mentionne_legault = map_int(mentions_screen_name, ~  "francoislegault" %in% .x) ,
    mentionne_vadeboncoeur = map_int(mentions_screen_name, ~  "Vadeboncoeur_Al" %in% .x)) %>%
  group_by(date) %>%
  filter(date >= lubridate::ymd("20200401"))%>% 
  summarise(count = n(),
            mentionne_legault = sum(mentionne_legault),
            mentionne_vadeboncoeur = sum(mentionne_vadeboncoeur)
  )   %>% 
  complete(date = seq.Date(lubridate::ymd("20200401"), Sys.Date(), by = "day")) %>%
  mutate(count = replace_na(count, 0), 
         mentionne_legault = replace_na(mentionne_legault, 0),
         mentionne_vadeboncoeur = replace_na(mentionne_vadeboncoeur, 0)
  )

moyenne_legault <- mean(mentions$mentionne_legault)
moyenne_vadeboncoeur <- mean(mentions$mentionne_vadeboncoeur)

mentions %>% 
  ggplot(aes(x = date, y = mentionne_legault)) +
  geom_col() + 
  dviz.supp::theme_dviz_grid()+
  scale_y_continuous(breaks = scales::pretty_breaks(n =5) )+
  labs(
    title = "Nombre quotidien de tweets de @Aaron_Derfel mentionnant @francoislegault",
    subtitle= paste0("moyenne de ",  round(moyenne_legault,1), " par jour depuis le 1er avril")
  ) +
  xlab("Date")+
  ylab ("Nombre de tweets") +
  theme(legend.position="bottom") +
  expand_limits(y = 0)
```


C'est cependant le Dr Alain Vadeboncoeur  qui remporte la palme pour le nombre de mention en une seule journée, avec 60 mentions.  


```{r}
mentions %>% 
  ggplot(aes(x = date, y = mentionne_vadeboncoeur)) +
  geom_col() + 
  dviz.supp::theme_dviz_grid()+
  scale_y_continuous(breaks = scales::pretty_breaks(n =5) )+
  labs(
    title = "Nombre quotidien de tweets de @Aaron_Derfel mentionnant @vadeboncoeur_al",
    subtitle= paste0("moyenne de ",  round(moyenne_vadeboncoeur,1), " par jour depuis le 1er avril")
  ) +
  xlab("Date")+
  ylab ("Nombre de tweets") +
  theme(legend.position="bottom") +
  expand_limits(y = 0)
```



#  Un peu de NLP

Voici les mots les plus utilisés par Aaron Derfel, ainsi qu'un wordcloud parce que c'est la mode.

```{r}

# tokenize, remove stopwords
tidytwit <- twit %>% 
  select(text, date, status_id) %>% 
  unnest_tokens(word, text)  %>% 
  anti_join(stop_words)

# most popular words

tidytwit %>%
  count(word, sort = TRUE)  %>%
  filter(n > 190) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() + 
  dviz.supp::theme_dviz_grid() +
  labs(title = "Mots les plus souvents utilisés par @Aaron_Derfel")
  
```

```{r}

tidytwit %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```



# Un peu de sentiment analysis  



```{r}
tweet_sentiments <- tidytwit %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, status_id, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment_tweet = positive - negative) 

date_sentiments <- tweet_sentiments %>% group_by(date ) %>% summarise(sentiment_day = sum(sentiment_tweet))

ggplot(date_sentiments, aes(date, sentiment_day)) +
  geom_col(show.legend = FALSE)  + 
  dviz.supp::theme_dviz_grid() +
  labs(title = "ÉVolution du sentiment global des tweets de @Aaron_Derfel")+ 
  xlab("Date")+
  ylab ("Sentiment net (mots positifs - mots négatifs")


```

Mais qu'a-t-il dit lors de cette journée si positive?
```{r}

# most positive day
date_sentiments %>% filter(sentiment_day == max(sentiment_day) ) %>% select(date) %>% 
  inner_join(tweet_sentiments) %>% 
  left_join(twit) %>%
  select(date, status_id, sentiment_tweet, negative, positive, text)  %>%
  knitr::kable()
```


Quel est le tweet le plus négatif ?

```{r}


# most negative tweets
# 
# tweet_sentiments %>% filter(sentiment_tweet == min(sentiment_tweet)) %>%
#   inner_join(twit)%>% 
#   select(date, status_id, sentiment_tweet, negative, positive, text) %>%
#   head(1) %>%
#   pull(status_id) %>%
#   blogdown::shortcode('tweet', .)
```





Le graphique ci-bas montre l'évolution de 10 sentiments dans les tweet de Aaron.   Il semble plus positif et moins ressentir de peur depuis quelques jours.  hourra :)



```{r}
fun <- tidytwit %>% 
  inner_join( get_sentiments("nrc") ) %>%
  count(date, sentiment) %>%
  left_join(twit %>% group_by(date) %>% summarise(ntweet = n())) %>%
  group_by(date) %>%
  mutate(percent = n /sum(n)) %>%
  mutate(intensity = n /ntweet) %>%
  
  ungroup()

fun %>%
  ggplot(aes(x= date, y = intensity)) +
  geom_line() +
  geom_smooth() +
  facet_wrap(~ sentiment) + 
  dviz.supp::theme_dviz_grid() + 
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
  labs(
    title = "Evolution of @Aaron_Derfel tweet sentiment",
    subtitle = "Words expressing a given sentiment per tweet sents on a given day",
    caption = "Graph by @coulsim"
  ) +
  xlab("date")+
  ylab ("Words per tweet")


```

that's it folks.
