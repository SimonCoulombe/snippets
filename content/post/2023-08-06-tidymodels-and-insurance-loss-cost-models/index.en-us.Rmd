---
title: tidymodels and insurance loss cost models
author: simon
date: '2023-08-06'
slug: index.en-us
categories:
  - category
  - subcategory
tags:
  - tag1
  - tag2
keywords:
  - tech
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  #dpi = 150,
  #fig.width = 5,
  #fig.height = 5,
  cache = FALSE # cache= TRUE leads to Error: path for html_dependency not found: error
)

ggplot2::theme_set(ggplot2::theme_minimal()) # this ggplot2 theme uses roboto condensed font, which works well with the font used for the whole document.
options(ggplot2.discrete.fill  = function() scale_fill_viridis_d() )
options(ggplot2.continuous.fill  = function() scale_fill_viridis_c())
options(ggplot2.discrete.colour = function() scale_color_viridis_d())
options(ggplot2.continuous.colour = function() scale_color_viridis_c())
ggplot2::update_geom_defaults("point", ggplot2::aes(color = "midnightblue"))
ggplot2::update_geom_defaults("line", ggplot2::aes(color = "midnightblue"))
ggplot2::update_geom_defaults("rect", ggplot2::aes(color = "midnightblue"))
options(scipen=999)

library(insuranceData)
library(tidyverse)
library(tidymodels)
library(bonsai)
library(xgboost)
library(mgcv)
library(lightgbm)
library(doMC)
library(gt)
library(vip)
library(broom)
library(statmod) #pour glm(family = tweedie)
library(bonsai)
tidymodels::tidymodels_prefer()
```

# Objective   

I built my first tweedie regression model [three years ago](https://www.simoncoulombe.com/2020/03/tweedie-vs-poisson-gamma/).  At that time I used the {recipes} and {rsample} packages from {tidymodels}, but I did the actual modelling outside of the {tidymodels} universe.  

Today I want to revisit this data, but this time I want to do everything using the {tidymodels} approach.  

Things I would like to do:   

* use {rsample}   do a "grouped" train/test data split.  This is often required when using insurance data because a given policy will have multiple "rows" and I want all rows for a given policy to be either in train or test rather than spread all over the place.  
* use {recipes} to replace high-cardinality variables with embedded values.  
* create `poisson`, `gamma` and `tweedie` regressions.  
* create models using `GLM`, `GAM`, `xgboost` and `lightgbm`.    
* evaluate multiple model types (glm, boosted trees) and multiple feature engineering "sets"  easily using workflow sets.     

Let's get started!    

# The data   

Copying from my 2020 post, here is the description of the data:   


#  Data  

The dataCar data from the `insuranceData` package.  It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005.   It originally came with the book [Generalized Linear Models for Insurance Data (2008)](http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/research/books/GLMsforInsuranceData).

The `exposure` variable  represents the "number of year of exposure" and is used as the offset variable.  It is bound between 0 and 1.   

Finally, the independent variables are as follow:  

* `veh_value`, the vehicle value in tens of thousand of dollars,  
* `veh_body`, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,  
* `veh_age`, 1 (youngest), 2, 3, 4,   
* `gender`, a factor with levels F M,   
* `area` a factor with levels A B C D E F,   
* `agecat` 1 (youngest), 2, 3, 4, 5, 6  

The presence of a claim is indicated by the `clm` (0 or 1) , which indicates that there is at least one claim.  We will rename the variable to `has_claim`.

The dollar amount of the claims is  `claimcst0`, which we will rename to `dollar_loss`.  We will divide it by the exposure to obtain `annual_loss`.

The `annual_loss` variable is what we actually want to model.  In insurance, it is typically modelled directly using a `tweedie`regression, or indirectly by multiplying the output of two models, one predicting  the frequency of claims  (poisson regression)  and the other predicting the severity of claims (gamma regression).

Note: when using the frequency*severity approach, we will use the `has_claim` variable in the Poisson (frequency) model instead of `numclaims` (actual number of claims) because we don't have the breakdown of value of claims when there is more than one.  In practice, this means modelling "a lower frequency and a higher severity" than reality, but the overall predicted annual loss will be the same.



FUN:  I am going to create a fake policy_id column, which has a 10% chance of being the same as the row above it.  This is to represent that a policy_id can have multiple records.  I will want all records for a given policy to be in the same resample.   


FUN#2:  I am also going to create a factor `has_claim_fct` because parsnip want classification models to work on factors, not integers.   

Fun#3: I am also going to create `exposure_weight`, which is the result of hardhat::importance_weights(exposure).   I *think* this will often allow us to pass weights to the tidymodels use use_case_weights.  

```{r}
set.seed(42)

data(dataCar)

# claimcst0 = claim amount in dollars (0 if no claim)
# clm = 0 or 1 = has a claim yes/ no  
#  numclaims = number of claims  0 , 1 ,2 ,3 or 4).       
# we use clm because the corresponding dollar amount is for all claims combined.  
mydb <- dataCar %>%
  select(has_claim = clm, dollar_loss= claimcst0, exposure, veh_value, veh_body,
         veh_age, gender, area, agecat) %>% 
  mutate(
    annual_loss = dollar_loss / exposure,
    policy_id =1,
    random = runif(nrow(.)),
    has_claim_fct = factor(has_claim),
    exposure_weight = hardhat::importance_weights(exposure) 
  )


# ugly and slow, but this is what I could come up with quickly
for (i in seq(from =2, to =nrow(mydb))){
  if (mydb[i,"random"]< 0.2 ){
    mydb[i,"policy_id"] <-  mydb[i-1,"policy_id"] 
  } else{
    mydb[i,"policy_id"] <-  mydb[i-1,"policy_id"] +1
  }
}

mydb %>% count(policy_id) %>% count(n) %>% gt(caption="most policy_id have only 1 record in the dataset, but a few have 2,3 and even 7")
```


# grouped and stratified  train/test split and rsamples     

We are going to use recipes::group_initial_split() to make sure that a given policy_id always ends up in the same resample.
I am also going to use the `strata` parameter to ensure that distribution of annual_loss in all resamples in the same:

```{r}
set.seed(123)

try(my_split <- group_initial_split( mydb, group = policy_id, prop= 3/4, strata = annual_loss))
```
oooh !   our first error.:  "`strata` must be constant across all members of each `group`.".  I'm going to calculate an average annual_loss over all records for a given policy_id.  I would love to be able to weight records by exposure (to have the same total exposure  and average annual_loss in all resamples), but I don't think that's possible.   

here we go.

```{r}
mydb <- mydb %>%
  group_by(policy_id) %>%
  mutate(policy_annual_loss = sum( dollar_loss) / sum(exposure)) %>%
  ungroup()

set.seed(123)
my_split <- group_initial_split( mydb, group = policy_id, prop= 3/4, strata = policy_annual_loss) # this works!  

my_train <- training(my_split)
my_test  <- testing(my_split)

```

are any policy_id split between train and test?    No, and this is great:   

```{r}
both <- bind_rows(
  my_train %>% mutate(type = "train"),
  my_test %>% mutate(type = "test"),
)


both %>%
  distinct(policy_id, type) %>% 
  count(policy_id) %>%
  count(n) %>%
  gt(caption = "all policy_id are seen in only 1 type (either train or test), never both")



```
Is the proportion of records 3/4 train and 1/4 test?  


```{r}
both %>% 
  count(type) %>%
  mutate(pct = n/sum(n)) %>% 
  gt(caption="proportion of records in train/test is exactly  75% vs 25%, as expected")
```
however, we would actually would have like thge exposure to have been spread 75%/25%. This is not the case since we couldnt weight records in the group_initial_split() function:   

```{r}
both %>% 
  group_by(type) %>% 
  summarise(exposure = sum(exposure)) %>%
  mutate(pct = exposure/sum(exposure)) %>%
  gt(caption="exposure is NOT split exactly  75-25 because we couldnt weight records when splitting")
```
We tried to stratify our groups using the policy_annual_loss variable. However, some policies have more exposure than other.  This means that the average annual loss for the train/test won't be identical.  How different are they? Quite a bit, actually:  

```{r}
both %>% 
  group_by(type) %>% 
  summarise(overall_annual_loss = sum(dollar_loss)/ sum(exposure)) %>%
  ungroup() %>% 
  gt(caption="overall annuall loss (dollar loss per year ) isnt the same in both train and test")
```
The overall  frequency should also be different between train and test.  It shouldnt be as different as the overall annual loss because the number of claims (0 or 1) is less volatile than the dollar amount of the claim.   


```{r}
both %>% 
  group_by(type) %>% 
  summarise(overall_frequency = sum(has_claim)/ sum(exposure)) %>%
  ungroup() %>% 
  gt(caption="overall frequency (claims per year) is less different  between train and test  than the overall annual loss")
```

to create folds ( I think they are called resamples in tidymodels), we use the group_vfold_cv, again stratified over policy_annual_loss   

```{r}
set.seed(234)
train_resamples <- group_vfold_cv(my_train,
                                  group="policy_id",
                                  strata = policy_annual_loss,
                                  v= 5)
```

I won't check the overall annual loss and overall frequency for each resamples, but the volatility here would be interesting to look at.   


edit: this blog post by Max Kuhhn indicates they are looking into weighting stuff.. https://www.tidyverse.org/blog/2022/05/case-weights/



# set up parallel processing    

we are going to run a lot of models when fitting models on multiples resamples and multiple hyperparameter sets.  tune_grid()  can use multiple cores if we tell it to:    

```{r}
doMC::registerDoMC(cores = 6)
```

# Logistic Regression  


Let's just pretend everyone has the same exposure and create a simple logistic regression on whether or not a record has a claim.  we'll look at more complicated stuff (poisson, gamma, tweedie) later.  

Here's the formula:  

```{r}
my_has_claim_formula <- as.formula("has_claim_fct ~ veh_value + veh_body + veh_age +  gender +  area +  agecat")
```


## Single models  



### GLM unweighted    
The code below to generate the specification for a  GLM logistic regression was generated automatically  by running `parsnip::parsnip_addin()`, selecting `classification`,  `logistic_reg (glm)` and unchecking `tag parameters for tuning (if any)` then clicking the gren `write specification code` button.   



```{r}
logistic_glm_spec <- 
  parsnip::logistic_reg() %>%
  parsnip::set_engine("glm")


logistic_glm_fit <-logistic_glm_spec %>%
  parsnip::fit(
    formula = my_has_claim_formula,
    data = my_train)

logistic_glm_fit %>%
  extract_fit_engine() %>% 
  summary()
```


is this the same output as directly running the normal glm?  
yes!
```{r}
logistic_glm_direct_fit <- glm(my_has_claim_formula,    family = "binomial", data = my_train) 

tidy(logistic_glm_fit) %>%  select(term, estimate_logistic_glm_fit = estimate) %>%
  left_join(tidy(logistic_glm_direct_fit) %>% select(term, estimate_logistic_glm_direct_fit = estimate)) 


```


### GLM weighted   
what if we want weights?   apparently we can use add_case_weights(), but this only work for workflows: 
```{r}

weighted_logistic_glm_wf <-   workflow() %>% 
  add_case_weights(exposure_weight) %>% 
  add_formula(my_has_claim_formula) %>%
  add_model(logistic_glm_spec)

weighted_logistic_glm_wf_fit <-  weighted_logistic_glm_wf%>% 
  fit(data = my_train)

weighted_logistic_glm_wf_fit
```
is it the same a directly modelling outside tidymodels?   
```{r}
weighted_logistic_glm_direct_fit <- glm(my_has_claim_formula,    family = "binomial", data = my_train, weights = exposure)

```

let's compare estimates for tidymodels/direct  for  unweighted logistic regression
```{r}
tidy(weighted_logistic_glm_wf_fit) %>% select(term, estimate_weighted_logistic_glm_wf_fit = estimate) %>%
  left_join(tidy(weighted_logistic_glm_direct_fit) %>% select(term, estimate_weighted_logistic_glm_direct_fit = estimate)) 
```
yes!! 

### GAM (with splines) weighted     

here's how I define a logistic regression in mgcv::gam().  

let's go stratight to workflows since one day we'll want to use workflow sets anyway...


* Note 1:  the REML method is passed to set_engine because this option is specific to the mgcv package.    
* Note 2: for some reason, GAMs  need us  to specify the formula twice, once in add_model() and another time in add_formula.  Interesting:   I need to add the spline in the formula in add_model(my_has_claim_spline_formula), but not in add_formula(my_has_claim_formula).
more here : https://community.rstudio.com/t/how-to-define-smoothed-models-for-a-gam-using-tidymodels-and-recipe/144772/2
* Note 3:  I need to add `parametric=TRUE` to broom::tidy()  to get the mgcv parameters (https://broom.tidymodels.org/reference/tidy.gam.html). parametric=FALSE will return the fitted spline.    





```{r}

# parsnip_addin()

logistic_gam_spec <-
  gen_additive_mod() %>%
  set_engine('mgcv', method= "REML") %>%
  set_mode('classification')

my_has_claim_spline_formula <- as.formula('has_claim_fct ~ s(veh_value, bs= "tp") + veh_body + veh_age +  gender +  area +  agecat')


logistic_gam_wf <- workflow() %>%
  add_model(logistic_gam_spec, formula = my_has_claim_spline_formula) %>%  # need to add formula twice, and  in add_formula
  add_formula(my_has_claim_formula) %>%
  add_case_weights(exposure_weight)


logistic_gam_wf_fit <-  logistic_gam_wf%>% 
  fit(data = my_train )


logistic_gam_wf_fit %>% tidy(parametric = TRUE)

```
same result if I do it directly using mgcv::gam:

```{r}
mgcv::gam(
  formula = my_has_claim_spline_formula, 
  data = my_train, 
  weights = exposure,
  family = stats::binomial(link = "logit"),
  method="REML") %>% 
  broom::tidy(parametric= TRUE)
```


### XGBoost  weighted     


```{r}
logistic_xgb_spec <-
  boost_tree(
    tree_depth = 3,
    trees= 100,
    learn_rate = 0.1,
    min_n = 50,
    loss_reduction = 0,
    sample_size = 1.0,
    stop_iter = 50
  ) %>%
  set_engine('xgboost', nthread = 1) %>%
  set_mode('classification')

logistic_xgb_wf <- workflow() %>%
  add_model(logistic_xgb_spec) %>%
  add_formula(my_has_claim_formula) %>%
  add_case_weights(exposure_weight)

set.seed(345)
logistic_xgb_wf_fit <-  logistic_xgb_wf%>% 
  fit(data = my_train )


logistic_xgb_wf_fit %>% extract_fit_engine() %>% vip::vip()
```
is the variable importance similar to what I get directly?  

naaah.. ah well.
xgboost is hard to reproduce...
```{r}
my_recipe <- recipe(my_train ) %>%
  update_role(all_of(labels(terms(my_has_claim_formula))), new_role = "predictor") %>%
  update_role(has_claim, new_role= "outcome") %>% 
  update_role(exposure, new_role = "weight") %>%
  step_dummy(all_nominal_predictors()) %>%
  step_select(has_role(c("predictor", "outcome", "weight")))

prepped_recipe <- prep(my_recipe) 
baked_train <- bake(prepped_recipe, my_train)

my_params <- list(min_child_weight = 50,
                  max_depth = 3,
                  eta = 0.1, 
                  gamma = 0, 
                  subsample = 1.0, 
                  nthread = 1)
xgtrain <- xgboost::xgb.DMatrix(
  data = as.matrix(baked_train %>% select(-has_claim, -exposure)),
  label = baked_train$has_claim,
  weight = baked_train$exposure
)

set.seed(345)
direct_xgb_fit <- xgboost::xgb.train(
  data = xgtrain,
  params = my_params,
  nrounds = 100
)
vip::vip(direct_xgb_fit)
```


### lightgbm unweighted   

```{r}
logistic_lightgbm_spec <-
  boost_tree(
    trees= 100
  ) %>%
  set_engine('lightgbm') %>%  # num_leaves = tune()
  set_mode('classification')

logistic_lightgbm_wf <- workflow() %>%
  add_model(logistic_lightgbm_spec) %>%
  add_formula(my_has_claim_formula) 


set.seed(345)
logistic_lightgbm_wf_fit <-  logistic_lightgbm_wf%>% 
  fit(data = my_train ) # Case weights are not enabled by the underlying model implementation.


logistic_lightgbm_wf_fit %>% extract_fit_engine() %>% vip::vip()
```

### lightgbm weighted   (not implemented in tidymodels)




TL;DR: `Case weights are not enabled by the underlying model implementation.`

```{r}
try(
  workflow() %>%
    add_model(logistic_lightgbm_spec) %>%
    add_formula(my_has_claim_formula)  %>% 
    add_case_weights(exposure_weight) %>% 
    fit(data = my_train ) 
)

```




## Use workflow_set to fit models  on all resamples   


we are going to define all the models at once using a single preprocessor (a formula, could have been a recipe) and a list of model specifications.  Then we apply fit_resamples() to fit all workflows to all resamples.  

note:  update_workflow_model() **exists** https://github.com/tidymodels/workflowsets/issues/64
it is used to add the formula to the GAM model inside a workflow_set.  

```{r}
all_workflows <- 
  workflow_set(
    preproc = list("formula"= my_has_claim_formula ),
    models = list(
      logistic_glm = logistic_glm_spec,
      logistic_gam = logistic_gam_spec,
      logistic_xgboost = logistic_xgb_spec
    ),
     case_weights = exposure_weight
  )

all_workflows <- update_workflow_model(all_workflows,
                      i =  "formula_logistic_gam",
                      spec = logistic_gam_spec,
                      formula = my_has_claim_formula)

# Workflows can take special arguments for the recipe (e.g. a blueprint) or a model (e.g. a special formula). However, when creating a workflow set, there is no way to specify these extra components. update_workflow_model() and update_workflow_recipe() allow users to set these values after the workflow set is initially created. They are analogous to workflows::add_model() or workflows::add_recipe().
      
all_workflows2 <- 
  all_workflows %>%
  workflow_map(resamples = train_resamples,
               fn = "fit_resamples",
               verbose = TRUE)
```



```{r}
rank_results(all_workflows2, rank_metric = "roc_auc")
```


```{r}
autoplot(all_workflows2, metric="roc_auc")
```


## Adding hyperparameter tuning to the workflow_set   



```{r}
logistic_tuneable_xgb_spec <-
  boost_tree(
    trees= 100,
    learn_rate = 0.1,
    tree_depth = tune(),
    min_n = tune(),
    loss_reduction = tune(),
    sample_size = tune(),
    mtry= tune(),
    stop_iter = 50
  ) %>%
  set_engine('xgboost', nthread = 1) %>%
  set_mode('classification')

```



```{r}
all_workflows3 <- 
  workflow_set(
    preproc = list("formula"= my_has_claim_formula ),
    models = list(
      logistic_glm = logistic_glm_spec,
      logistic_gam = logistic_gam_spec,
      logistic_xgboost = logistic_xgb_spec,
      logistic_tuneable_xgb = logistic_tuneable_xgb_spec
    ),
     case_weights = exposure_weight
  )

all_workflows3 <- update_workflow_model(all_workflows3,
                      i =  "formula_logistic_gam",
                      spec = logistic_gam_spec,
                      formula = my_has_claim_formula)

# Workflows can take special arguments for the recipe (e.g. a blueprint) or a model (e.g. a special formula). However, when creating a workflow set, there is no way to specify these extra components. update_workflow_model() and update_workflow_recipe() allow users to set these values after the workflow set is initially created. They are analogous to workflows::add_model() or workflows::add_recipe().
      
all_workflows4 <- 
  all_workflows3 %>%
  workflow_map(resamples = train_resamples,
               fn = "tune_grid",
               verbose = TRUE)
```

this time we have a lot more of boosted trees because `logistic_tuneable_xgb` goes through a grid of 10 sets of hyperparameters. 

```{r}
autoplot(all_workflows4, metric="roc_auc")
```

```{r}
rank_results(all_workflows4, rank_metric = "roc_auc")
```

Here the best model is the "logistic_xgboost", the one where I hand-picked hyperparameters.  Let's pretend that the best model was one of the 10 sets of hyperparameters from "logistic_tuneable_xgb".  We could extract that workflow, then show the best 5 results and finally select_best() hyperparameters and finalise the `logistic_tuneable_xgb` by running the model on the whole training set with the best hyperparameters.    
```{r}
logistic_tuneable_xgb_wf_result <- 
  all_workflows4 %>%
  extract_workflow_set_result("formula_logistic_tuneable_xgb")

logistic_tuneable_xgb_wf_result %>% show_best(metric = "roc_auc")
```

```{r}
logistic_tuneable_xgb_wf_fit <- all_workflows4 %>%
  extract_workflow("formula_logistic_tuneable_xgb") %>%
  finalize_workflow(select_best(logistic_tuneable_xgb_wf_result, metric = "roc_auc")) %>%
  last_fit(split= my_split)

logistic_tuneable_xgb_wf_fit
```
```{r}
preds <- collect_predictions(logistic_tuneable_xgb_wf_fit)
test_with_preds <- augment(logistic_tuneable_xgb_wf_fit)
```


```{r}
test_with_preds %>%
  ggplot(aes(x=.pred_1)) +
  geom_histogram()
```

the mean of test_with_preds$has_claim is not the same as  test_with_preds$.pred_1.. shoot. maybe it wasnt done learning?

```{r}
mean(test_with_preds$has_claim)
```
```{r}
mean(test_with_preds$.pred_1)
```

anyway, enough for today.

# Tweedie regression  

alright that was fun, let's try tweedie.   


```{r}
my_annual_loss_formula <- as.formula("annual_loss ~ veh_value + veh_body + veh_age +  gender +  area +  agecat")
```

## Single models   


### GLM weighted   


glm  tweedie model in tidymodels:


stackoverflow adding gamma to glm regression: https://stackoverflow.com/questions/66024469/glm-family-using-tidymodels




```{r}
tweedie_glm_spec <- 
  parsnip::linear_reg(mode = "regression") %>%
  parsnip::set_engine("glm", family=tweedie(var.power=1.1, link.power=0))


tweedie_glm_wf <-   workflow() %>% 
  add_case_weights(exposure_weight) %>% 
  add_formula(my_annual_loss_formula) %>%
  add_model(tweedie_glm_spec)

tweedie_glm_wf_fit <-  tweedie_glm_wf %>% 
  fit(data = my_train)

tweedie_glm_wf_fit %>%  tidy()
```



tweedie model when calling glm directly. identique. yes!!
```{r}
tweedie_fit <- 
  glm(formula =  my_annual_loss_formula,
      family=tweedie(var.power=1.1, link.power=0),
      weights = exposure,
      data = my_train)

tweedie_fit %>% tidy()
```


### GAM (with splines) weighted     


```{r}
my_annual_loss_spline_formula <- as.formula('annual_loss ~ s(veh_value, bs= "tp") + veh_body + veh_age +  gender +  area +  agecat')
```


```{r}

tweedie_gam_spec <-
  gen_additive_mod() %>%
  set_engine('mgcv', method= "REML", family = Tweedie(p = 1.1, link = "log")) %>%
  set_mode('regression')

tweedie_gam_wf <- workflow() %>%
  add_model(tweedie_gam_spec, formula = my_annual_loss_spline_formula) %>%  # need to add formula twice, and  in add_formula
  add_formula(my_annual_loss_formula) %>%
  add_case_weights(exposure_weight)


tweedie_gam_wf_fit <-  tweedie_gam_wf%>% 
  fit(data = my_train )


tweedie_gam_wf_fit %>% tidy(parametric = TRUE)

```
same result if I do it directly using mgcv::gam?

```{r}
mgcv::gam(
  formula = my_annual_loss_spline_formula, 
  data = my_train, 
  weights = exposure,
  family = Tweedie(p = 1.1, link = "log"),
  method="REML") %>% 
  broom::tidy(parametric= TRUE)
```

YES!! ESTI QUE JE SUIS BON AHAH!





### XGBoost  weighted     

```{r}
tweedie_xgb_spec <-
  boost_tree(
    tree_depth = 3,
    trees= 100,
    learn_rate = 0.1,
    min_n = 50,
    loss_reduction = 0,
    sample_size = 1.0,
    stop_iter = 50
  ) %>%
  set_engine('xgboost', nthread = 1, objective = "reg:tweedie",eval_metric="tweedie-nloglik@1.1", tweedie_variance_power = 1.1) %>% ##https://www.kaggle.com/code/olehmezhenskyi/tweedie-xgboost
  set_mode('regression') 


tweedie_xgb_wf <- workflow() %>%
  add_model(tweedie_xgb_spec) %>%
  add_formula(my_annual_loss_formula) %>%
  add_case_weights(exposure_weight)

set.seed(456)
tweedie_xgb_wf_fit <-  tweedie_xgb_wf%>% 
  fit(data = my_train )


tweedie_xgb_wf_fit %>% extract_fit_engine() %>% vip::vip()
```

# TODO: Poisson regression  

## Single models   

### GLM weighted   
### GAM (with splines) weighted     
### xgboost  weighted     

# TODO: Gamma regression   
## Single models   
### GLM weighted   
### GAM (with splines) weighted     
### xgboost  weighted     



