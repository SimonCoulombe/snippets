---
title: "Bayesian optimization of xgboost hyperparameters for a Poisson regression in R"
author: "Simon Coulombe"
date: 2019-01-09
slug: "bayesian"
output:
  blogdown::html_page:
    toc: false
categories: ["R"]
tags: ["bayesian", "optimization", "rstats", "mlrMBO", "caret", "mlr" ,"poisson", "rBayesianOptimization"]
---
  
  
```{r setup, include =F}
# TODO : valider ceci : Chunk options must be written in one line; no line breaks are allowed inside chunk options;
# https://yihui.name/knitr/options/
knitr::opts_chunk$set(collapse = FALSE,
                      warning = FALSE,
                      error = FALSE,
                      message = FALSE,
                      fig.align= "center",
                      fig.width = 10,
                      highlight = TRUE,
                      cache = FALSE,
                      cache.lazy = FALSE) # fixes long vector not supported quand on cache des gros éléments https://stackoverflow.com/questions/39417003/long-vectors-not-supported-yet-error-in-rmd-but-not-in-r-script
```

# Introduction  

Ratemaking models in insurance routinely use Poisson regression to model the frequency of auto insurance claims.  They usually are GLMs but some insurers are moving towards GBMs, such as `xgboost`.

`xgboost`has multiple hyperparameters that can be tuned to obtain a better predictive power.  There are multiple ways to tune these hyperparameters.  In order of efficiency are the grid search, the random search and the bayesian optimization search.  

In this post, we will  compare the results of  xgboost hyperparameters for a Poisson regression in R using a random search versus a bayesian search.  Two packages wills be compared for the bayesian approach:  the `mlrMBO` package and the `rBayesianOptimization` package.  

We will model the number of auto insurance claims based on characteristics of the car and driver, while offsetting for exposure.   The data comes from the `insuranceData` package.  

For most model types, `mlrMBO` can be used in combination with the `mlr` package to find the best hyperparameters directly.  As far as I know the `mlr` package doesnt [handle poisson regression](https://github.com/mlr-org/mlr/issues/515), so we will have to create our own function to maximise.


I tried the `rBayesianOptimization` package after being inspired by this post from  [Max Kuhn](http://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html) from 2016.  I do not recommend using this package because it  [sometimes recycles hyperparameters](https://github.com/yanyachen/rBayesianOptimization/issues/4) and hasnt been updated on github since 2016.

**Keep your eyes peeled**: Max Kuhn (@topepos) [said that tidymodels might do this in the first half of 2019](https://twitter.com/topepos/status/1075151561863692290).


```{r librairies}
library(xgboost)
library(insuranceData) # example dataset https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf
library(tidyverse) # for data wrangling
library(rBayesianOptimization) # to create cv folds and for bayesian optimisation
library(mlrMBO)  # for bayesian optimisation
library(skimr) # for summarising databases
library(purrr) # to evaluate the loglikelihood of each parameter set in the random grid search
require("DiceKriging") # mlrmbo requires this
require("rgenoud") # mlrmbo requires this

``` 


# Preparing the data

First, we load the dataCar data from the `insuranceData` package.  It contains 67 856 one-year vehicle insurance policies taken out in 2004 or 2005.   

The dependent variable is  `numclaims`, which represents the number of claims.    

The `exposure` variable  represents the "number of year of exposure" and is used as the offset variable.  It is bounded between 0 and 1.   

Finally, the independent variables are as follow:  

* `veh_value`, the vehicle value in tens of thousand of dollars,  
* `veh_body`, y vehicle body, coded as BUS CONVT COUPE HBACK HDTOP MCARA MIBUS PANVN RDSTR SEDAN STNWG TRUCK UTE,  
* `veh_age`, 1 (youngest), 2, 3, 4,   
* `gender`, a factor with levels F M,   
* `area` a factor with levels A B C D E F,   
* `agecat` 1 (youngest), 2, 3, 4, 5, 6  



```{r prep_dataCar}
# load insurance data
data(dataCar)
mydb <- dataCar %>% select(numclaims, exposure, veh_value, veh_body,
                           veh_age, gender, area, agecat)
label_var <- "numclaims"  
offset_var <- "exposure"
feature_vars <- mydb %>% 
  select(-one_of(c(label_var, offset_var))) %>% 
  colnames()

skimr::skim(mydb ) %>% 
  skimr::kable()
```

Insurance ratemaking often requires monotonous relationships.  In our case, we will arbitrarily force the number of claims to be non-increasing with the age of the vehicle.

The code below imports the data, one-hot encodes dummy variables,  converts the data frame to a xgb.DMatrix for xgboost consumption, sets the offset for exposure, sets the constraints and defines the 3 folds we will use for cross-validation.




```{r}
# one hot encoding of categorical (factor) data
myformula <- paste0( "~", paste0( feature_vars, collapse = " + ") ) %>% 
  as.formula()

dummyFier <- caret::dummyVars(myformula, data=mydb, fullRank = TRUE)
dummyVars.df <- predict(dummyFier,newdata = mydb)
mydb_dummy <- cbind(mydb %>% select(one_of(c(label_var, offset_var))), 
                    dummyVars.df)
rm(myformula, dummyFier, dummyVars.df)

# get  list the column names of the db with the dummy variables
feature_vars_dummy <-  mydb_dummy  %>% 
  select(-one_of(c(label_var, offset_var))) %>% 
  colnames()

# create xgb.matrix for xgboost consumption
mydb_xgbmatrix <- xgb.DMatrix(
  data = mydb_dummy %>% select(feature_vars_dummy) %>% as.matrix, 
  label = mydb_dummy %>% pull(label_var),
  missing = "NAN")

#base_margin: apply exposure offset 
setinfo(mydb_xgbmatrix,"base_margin", 
        mydb %>% pull(offset_var) %>% log() )

# a fake constraint, just to show how it is done.  
#Here we force "the older the car, the less likely are claims"
myConstraint   <- data_frame(Variable = feature_vars_dummy) %>%
  mutate(sens = ifelse(Variable == "veh_age", -1, 0))

# random folds for xgb.cv
cv_folds = rBayesianOptimization::KFold(mydb_dummy$numclaims, 
                                        nfolds= 3,
                                        stratified = TRUE,
                                        seed= 0)
```


# Example 1: Optimize hyperparameters using a random search   (non bayesian)

We will start with a quick example of random search.  

I don't use caret for the random search  [because it has a hard time with poisson regression](https://github.com/topepo/caret/issues/861).

First, we generate 20 random sets of hyperparameters.  I will force `gamma = 0` for half the sets.I will also hardcode an extra set of parameters named `simon_params` because I find this combination is often a good starting point.

```{r random_grid_search, echo=T}
# generate hard coded parameters
simon_params <- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0,
                           nrounds = 200) %>% 
  as_tibble()
# generate 20 random models
how_many_models <- 20
max_depth <-        data.frame(max_depth = floor(runif(how_many_models)*5 ) + 3)  # 1 à 4
colsample_bytree <- data.frame(colsample_bytree =runif(how_many_models) * 0.8 + 0.2)  # 0.2 à 1
subsample <-        data.frame(subsample =runif(how_many_models) * 0.8 + 0.2) # 0.2 à 1
min_child_weight <- data.frame(min_child_weight = floor(runif(how_many_models) * 10) + 1) # 1 à 10
eta <-              data.frame(eta = runif(how_many_models) * 0.06 + 0.002) # 0.002 à 0.062
gamma <-            data.frame(gamma =c(rep(0,how_many_models/2), runif(how_many_models/2)*10)) # 0 à 10
nrounds <-          data.frame(nrounds = rep(2e2,how_many_models)) # max 200

random_grid <-max_depth %>%
  bind_cols(colsample_bytree ) %>%
  bind_cols(subsample) %>%
  bind_cols(min_child_weight) %>%
  bind_cols(eta) %>%
  bind_cols(gamma) %>%
  bind_cols(nrounds)  %>% as_tibble()
# combine random and hardcoded parameters
df.params <- simon_params %>%  bind_rows(random_grid) %>%
  mutate(rownum = row_number(),
         rownumber = row_number())
list_of_param_sets <- df.params %>% nest(-rownum)
```

Here are the hyperparameters that will be tested:

```{r}
kable(df.params)
```

Evaluate these 21 models with 3 folds and retourn the loglikelihood of each models:  



```{r, eval =F}
start <- Sys.time()
random_grid_results <- list_of_param_sets %>% 
  mutate(booster = map(data, function(X){
    message(paste0("model #",       X$rownumber,
            " eta = ",              X$eta,
            " max.depth = ",        X$max_depth,
            " min_child_weigth = ", X$min_child_weight,
            " subsample = ",        X$subsample,
            " colsample_bytree = ", X$colsample_bytree,
            " gamma = ",            X$gamma, 
            " nrounds = ",          X$nrounds))
set.seed(1234)

cv <- xgb.cv(
  params = list(
    booster = "gbtree",
    eta = X$eta,
    max_depth = X$max_depth,
    min_child_weight = X$min_child_weight,
    gamma = X$gamma,
    subsample = X$subsample,
    colsample_bytree = X$colsample_bytree,
    objective = 'count:poisson', 
    eval_metric = "poisson-nloglik"),
  data = mydb_xgbmatrix,
  nround = X$nrounds,
  folds=  cv_folds,
  monotone_constraints = myConstraint$sens,
  prediction = FALSE,
  showsd = TRUE,
  early_stopping_rounds = 50,
  verbose = 0)

function_return <- list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)], # l'itération qui a la plus haute moyenne de nloglik sur toutes les cv folds
                        Pred = 0)

message(paste0("Score :", function_return$Score))
return(function_return)})) %>%
  mutate(Score =  pmap(list(booster), function(X){X$Score })%>% unlist())


write_rds(random_grid_results, "temp_files/random_grid_results.rds")
stop <- Sys.time()
stop- start
```

```{r}
random_grid_results <- read_rds( "temp_files/random_grid_results.rds")

random_grid_results %>%
  mutate( hardcoded = ifelse(rownum ==1,TRUE,FALSE)) %>%
  ggplot(aes( x = rownum, y = Score, color = hardcoded)) + 
  geom_point() +
  labs(title = "random grid search")+
  ylab("loglikelihood")
```

# Example #2 Bayesian optimization using `mlrMBO`

This tutorial builds on the [mlrMBO vignette](https://mlrmbo.mlr-org.com/articles/mlrMBO.html)

First, we need to define the objective function that the bayesian search will try to maximise.  In this case we want to maximise the log likelihood of the out of fold predictions.

```{r}
# objective function: we want to maximise the log likelihood by tuning most parameters
obj.fun  <- smoof::makeSingleObjectiveFunction(
  name = "xgb_cv_bayes",
  fn =   function(x){
    set.seed(12345)
    cv <- xgb.cv(params = list(
      booster          = "gbtree",
      eta              = x["eta"],
      max_depth        = x["max_depth"],
      min_child_weight = x["min_child_weight"],
      gamma            = x["gamma"],
      subsample        = x["subsample"],
      colsample_bytree = x["colsample_bytree"],
      objective        = 'count:poisson', 
      eval_metric     = "poisson-nloglik"),
      data = mydb_xgbmatrix,
      nround = 30,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 10,
      verbose = 0)
    
    cv$evaluation_log[, max(test_poisson_nloglik_mean)]
  },
  par.set = makeParamSet(
    makeNumericParam("eta",              lower = 0.001, upper = 0.05),
    makeNumericParam("gamma",            lower = 0,     upper = 5),
    makeIntegerParam("max_depth",        lower= 1,      upper = 10),
    makeIntegerParam("min_child_weight", lower= 1,      upper = 10),
    makeNumericParam("subsample",        lower = 0.2,   upper = 1),
    makeNumericParam("colsample_bytree", lower = 0.2,   upper = 1)
  ),
  minimize = FALSE
)

```

After this, we generate the design, which are a set of hyperparameters that will be tested before starting the bayesian optimization.  Here we generate only 10 sets, but `mlrMBO`would normally generate 4 times the number of parameters.  I also force my  `simon_params` to be part of the design because I want to make sure at least one of of sets generated is good.  

```{r}
# generate an optimal design with only 10  points
des = generateDesign(n=10,
                     par.set = getParamSet(obj.fun), 
                     fun = lhs::randomLHS)  ## . If no design is given by the user, mlrMBO will generate a maximin Latin Hypercube Design of size 4 times the number of the black-box function’s parameters.
# i still want my favorite hyperparameters to be tested
simon_params <- data.frame(max_depth = 6,
                           colsample_bytree= 0.8,
                           subsample = 0.8,
                           min_child_weight = 3,
                           eta  = 0.01,
                           gamma = 0) %>% as_tibble()
#final design  is a combination of latin hypercube optimization and my own preferred set of parameters
final_design =  simon_params  %>% bind_rows(des)
# bayes will have 10 additional iterations
control = makeMBOControl()
control = setMBOControlTermination(control, iters = 10)
```

Run the bayesian search:  


```{r, eval =F}
# run this!
run = mbo(fun = obj.fun, 
          design = final_design,  
          control = control, 
          show.info = TRUE)
write_rds( run, "temp_files/run.rds")
```


```{r}
run <- read_rds( "temp_files/run.rds")
# print a summary with run
#run
# return  best model hyperparameters using run$x
# return best log likelihood using run$y
# return all results using run$opt.path$env$path
run$opt.path$env$path  %>% 
  mutate(Round = row_number()) %>%
  mutate(type = case_when(
    Round==1  ~ "1- hardcoded",
    Round<= 11 ~ "2 -design ",
    TRUE ~ "3 - mlrMBO optimization")) %>%
  ggplot(aes(x= Round, y= y, color= type)) + 
  geom_point() +
  labs(title = "mlrMBO optimization")+
  ylab("loglikelihood")
```

# Conclusion:  
Bayesian optimization does generate better models, but it might be overkill if you aren't participating in a kaggle.  The table below shows the log likelihood of the best model found using the random grid and the mlrMBO and compares it my `simon params`.  



```{r, echo = F}
hardcoded <- random_grid_results[1,] %>%
  pull(data) %>% .[[1]] %>%
  mutate(type = "simon params") %>%
  mutate(loglikelihood= random_grid_results[1,"Score"] %>% as.numeric())

best_random <- random_grid_results %>% 
  filter(Score == max(Score)) %>% pull(data) %>% .[[1]] %>% 
  mutate(type = "random_grid") %>%
  mutate(loglikelihood = max(random_grid_results$Score))

best_mlrMBO <- run$opt.path$env$path  %>% 
  filter(y == max(y))  %>%
  mutate(type = "mlrMBO") %>%
  mutate(loglikelihood= run$y ) %>%
  head(1)

hardcoded %>% 
  bind_rows(best_random) %>% 
  bind_rows(best_mlrMBO) %>%
  select(type,loglikelihood, max_depth, colsample_bytree, subsample,min_child_weight,eta, gamma) %>% kable
```

# Code
The code that generated this document is located at

https://github.com/SimonCoulombe/snippets/blob/master/content/post/2019-1-09-bayesian.Rmd

## mlrMBO further reading  
 
Here are some resources I used to build this post:  

* [Xgboost using MLR package](http://rstudio-pubs-static.s3.amazonaws.com/336732_52d1b0e682634b5eae42cf86e1fc2a98.html)  
* [Vignette: https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html](https://mlrmbo.mlr-org.com/articles/supplementary/machine_learning_with_mlrmbo.html) , how to use lrMBO with mlr. I dont think you can do poisson regression using mlr.
* [Parameter tuning with mlrHyperopt](https://www.r-bloggers.com/parameter-tuning-with-mlrhyperopt/)  
* [Train R ML models efficiently with mlr](https://www.kaggle.com/xanderhorn/train-r-ml-models-efficiently-with-mlr)  

# Appendix: (NOT RECOMMENDED) Bayesian optimization  using `rBayesianOptimization`


Here I show how to do an equivalent optimization using `rBaysianOptimization`.  I do not recommend using this package because it  [recycles hyperparameters](https://github.com/yanyachen/rBayesianOptimization/issues/4) and hasnt been updated on github since 2016.

First, we have to define a special function that return a list of two values that will be returned to the bayesian optimiser:  
  - "Score" should be the metrics to be maximized ,  
- "Pred" should be the validation/cross-valiation prediction for ensembling / stacking.  We can set it to 0 to save on memory.

This function will be named  `xgb_cv_bayes` : 
```{r}
xgb_cv_bayes <- 
  function(max_depth=4, 
           min_child_weight=1, 
           gamma=0,
           eta=0.01,
           subsample = 0.6,
           colsample_bytree =0.6,
           nrounds = 200, 
           early_stopping_rounds = 50 ){
    set.seed(1234)
    cv <- xgb.cv(params = list(
      booster = "gbtree",
      eta = eta,
      max_depth = max_depth,
      min_child_weight = min_child_weight,
      gamma = gamma,
      subsample = subsample,
      colsample_bytree = colsample_bytree,
      objective = 'count:poisson', 
      eval_metric = "poisson-nloglik"),
      data = mydb_xgbmatrix,
      nround = nrounds,
      folds=  cv_folds,
      monotone_constraints = myConstraint$sens,
      prediction = FALSE,
      showsd = TRUE,
      early_stopping_rounds = 50,
      verbose = 0)
    
    list(Score = cv$evaluation_log[, max(test_poisson_nloglik_mean)],
         Pred = 0)
  }
```

We then launch `BayesianOptimization`, specifying:    
  - the function to optimise (xgb_cv_bayes),  
- the hyperparameters (`bounds`) ,  
- the number of iterations, `n_iter`, 


The optimization function needs some points for initialisation.  We pass one of the two following parameters:   
  - `init_points` , start from scratch using this number of randomly generated hyperparameter sets, or 
- `init_grid_dt`  use the knowledge from a previous run.


##  rBayesianOptimization from scratch  
Here is an example  "from scratch". 

```{r bayes_scratch, collapse = FALSE, eval = F}
start <- Sys.time()
bayesian_results <- rBayesianOptimization::BayesianOptimization(
  FUN = xgb_cv_bayes,
  bounds = list(max_depth = c(2L, 10L),
                colsample_bytree = c(0.3, 1),
                subsample = c(0.3,1),
                min_child_weight = c(1L, 10L),
                eta = c(0.001, 0.03),
                gamma = c(0, 5)),
  init_grid_dt = NULL, init_points = 4, 
  n_iter = 7,
  acq = "ucb", kappa = 2.576, eps = 0.0,
  verbose = TRUE)
stop <- Sys.time()
stop- start
write_rds(bayesian_results, "temp_files/bayesian_results.rds")
```

```{r}
bayesian_results <- read_rds( "temp_files/bayesian_results.rds")
```



```{r}
kable(bayesian_results$History)
bayesian_results$History  %>% 
  mutate(type = case_when(
    Round<= 4  ~ "1- init_points",
    Round<= 11 ~ "2 -n_iter",
    TRUE ~ "wtf")) %>%
  ggplot(aes(x= Round, y= Value, color= type)) + geom_point()+
  labs(title = "bayesian from scratch")
```

## rBayesianOptimization resuming from previous runs

`init_grid_dt` is a data frame with the same columns as `bounds` plus a `Value`column which correspond to the log likelihood we calculated earlier.  


### ... resuming from random grid

Here is an example resuming from the random grid of example #1

```{r bayes_continued_randomgrid, collapse = FALSE}
init_grid <- random_grid_results$data %>% 
  bind_rows() %>% 
  add_column(Value = random_grid_results$Score)   %>% 
  select(max_depth, colsample_bytree, subsample, min_child_weight, eta, gamma, Value)
kable(init_grid)
```

```{r, eval = F}
bayesian_results_continued_from_randomgrid <- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt = init_grid, init_points = 0, 
                       n_iter = 5,
                       acq = "ucb", kappa = 2.576, eps = 0.0,
                       verbose = TRUE)
write_rds(bayesian_results_continued_from_randomgrid,
          "temp_files/bayesian_results_continued_from_randomgrid.rds")
```

```{r}
bayesian_results_continued_from_randomgrid <- read_rds(
  "temp_files/bayesian_results_continued_from_randomgrid.rds")
```


### ... resuming from previous rBayesianOptimization

We can also resume from a brevious rBayesianOptimization run using the `$History` value it returns.  In the example below, we will resume from the previous bayesian search that was itself build upon the random grid search.



```{r, collapse = FALSE}
init_grid <- bayesian_results_continued_from_randomgrid$History %>% select(-Round)
kable(init_grid)
```

```{r bayes_continued_bayesian, collapse= FALSE, eval=  F}
bayesian_results_continued_from_bayesian <- 
  BayesianOptimization(xgb_cv_bayes,
                       bounds = list(max_depth = c(2L, 10L),
                                     colsample_bytree = c(0.3, 1),
                                     subsample = c(0.3,1),
                                     min_child_weight = c(1L, 10L),
                                     eta = c(0.001, 0.03),
                                     gamma = c(0, 5)),
                       init_grid_dt =init_grid , init_points = 0,
                       n_iter = 5,
                       acq = "ucb", kappa = 2.576, eps = 0.0,
                       verbose = TRUE)
write_rds(bayesian_results_continued_from_bayesian, "temp_files/bayesian_results_continued_from_bayesian.rds")
```

```{r}
bayesian_results_continued_from_bayesian <- read_rds(
  "temp_files/bayesian_results_continued_from_bayesian.rds")
```


```{r results}
bayesian_results_continued_from_bayesian$History  %>% 
  mutate(type = case_when(
    Round==1  ~ "1- hardcoded",
    Round<= 21 ~ "2 -random grid ",
    Round <= 26 ~ "3 - rbayesianoptimization run #1 resuming from random grid",
    TRUE ~ "4 - rbayesianoptimization run #2 resuming from bayesian run 1")) %>%
  ggplot(aes(x= Round, y= Value, color= type)) + 
  geom_point()+ 
  labs(title = "rBayesianOptimization")+
  ylab("loglikelihood")
```


# Conclusion

The table below shows the loglikelihood and hyperparameters for the hardcoded `simon_param` model, and the best models found by random search, mlrMBO and rBayesianOptimisation.  


Bayesian optimization using mlrMBO and rBayesianOptimization both yield the best results.  The gain in loglikelihood above the "hardcoded" `simon_param` or the random search isnt that great, however, so it may not be necessary to implement `mlrMBO` in a non-kaggle setting.



```{r}
hardcoded <- random_grid_results[1,] %>%
  pull(data) %>% .[[1]] %>%
  mutate(type = "hardcoded") %>%
  mutate(loglikelihood= random_grid_results[1,"Score"] %>% as.numeric())
best_random <- random_grid_results %>% 
  filter(Score == max(Score)) %>% pull(data) %>% .[[1]] %>% 
  mutate(type = "random_grid") %>%
  mutate(loglikelihood = max(random_grid_results$Score))

best_mlrMBO <- run$opt.path$env$path  %>% 
  filter(y == max(y))  %>%
  mutate(type = "mlrMBO") %>%
  mutate(loglikelihood= run$y ) %>%
  head(1)
best_rBayesianOptimization <- 
  bayesian_results_continued_from_bayesian$History %>%
  filter(Value == max(Value)) %>%
  mutate(type = "rBayesianOptimization") %>%
  mutate(loglikelihood = max(bayesian_results_continued_from_bayesian$History$Value))
hardcoded %>% 
  bind_rows(best_random) %>% 
  bind_rows(best_mlrMBO) %>%
  bind_rows(best_rBayesianOptimization) %>%
  select(type,loglikelihood, max_depth, colsample_bytree, subsample,min_child_weight,eta, gamma) %>% kable
```
